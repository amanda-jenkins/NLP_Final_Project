# -*- coding: utf-8 -*-
"""FINAL_HF_Gemma_Work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CBWytrI-va_WrzXnf_dXXfGdI5PUTD5o
"""

# from huggingface_hub import login
# login()

# import kagglehub

# # Authenticate
# kagglehub.login() # This will prompt you for your credentials.
# # We also offer other ways to authenticate (credential file & env variables): https://github.com/Kaggle/kagglehub?tab=readme-ov-file#authenticate

# !pip install datasets
# !pip install trl

# !pip install langchain_community

# !pip install evaluate

# !pip install transformers

!pip install transformers # Ensure transformers is installed
from transformers import AutoTokenizer, AutoModelForCausalLM # Import necessary classes

access_token = "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"
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it", token=access_token)
model = AutoModelForCausalLM.from_pretrained("google/gemma-2b-it", token=access_token)

# Sample provided model code for debugging purposes

input_text = "What is a car?"
input_ids = tokenizer(input_text, return_tensors="pt")

outputs = model.generate(**input_ids) # Now 'model' should have the 'generate' method
print(tokenizer.decode(outputs[0]))

!pip install -U keras-nlp
!pip install -U keras

import keras
import keras_nlp
import numpy as np

!pip install rouge_score

!pip install bert-score

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

"""# CNN / Dailymail Dataset Summarization + VADER Analysis via GEMMA 2B-IT

"""

from bert_score import score as bert_score


def ensure_full_sentence(text):

    sentences = text.split('. ')
    if len(sentences) > 1 and not text.endswith('.'):
        text = '. '.join(sentences[:-1]) + '.'
    return text.strip()


def generate_summary(article, max_new_tokens=95, min_new_tokens=30, max_words=150):

    # tokenize the input
    inputs = tokenizer.encode("summarize: " + article, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(
        inputs,
        max_new_tokens=max_new_tokens,
        min_new_tokens=min_new_tokens,
        num_beams=2,
        early_stopping=True,
        length_penalty=2.0,
        no_repeat_ngram_size=3
    )
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)


    summary_words = summary.split()
    if len(summary_words) > max_words:
        summary = " ".join(summary_words[:max_words])


    summary = ensure_full_sentence(summary)
    return summary

def calculate_rouge_scores(reference, generated_summary):
    """
    Calculates ROUGE-1, ROUGE-2, and ROUGE-L scores.
    """
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, generated_summary)
    return scores

def calculate_bert_scores(reference, generated_summary):
    """
    Calculates BERT Precision, Recall, and F1 Scores.
    """
    P, R, F1 = bert_score([generated_summary], [reference], lang="en", rescale_with_baseline=True)
    return P.item(), R.item(), F1.item()

# calculate VADER sentiment scores
def calculate_vader_scores(text):
    """
    Returns the VADER sentiment scores for a given text.
    """
    return analyzer.polarity_scores(text)

# path to dataset
test_path = '/content/test.csv'

# load the dataset
test_df = pd.read_csv(test_path, on_bad_lines='skip', engine='python')

print(test_df.head())

# loop through summaries range
for index in range(0, 80):
    sample_article = test_df['article'][index]
    reference_summary = test_df['highlights'][index]

    # validate article content
    if pd.isna(sample_article) or sample_article.strip() == "":
        print(f"Summary {index}: Article is empty or NaN!")
        continue

    generated_summary = generate_summary(sample_article)

    rouge_scores = calculate_rouge_scores(reference_summary, generated_summary)

    bert_precision, bert_recall, bert_f1 = calculate_bert_scores(reference_summary, generated_summary)

    generated_vader = calculate_vader_scores(generated_summary)
    reference_vader = calculate_vader_scores(reference_summary)

    print(f"Summary {index}:")
    print(f"Generated Summary:\n{generated_summary}")
    print(f"\nVADER Scores for Generated Summary: {generated_vader}")
    print(f"VADER Scores for Reference Summary: {reference_vader}")
    print(f"\nROUGE Scores:")
    print(f"ROUGE-1: Precision: {rouge_scores['rouge1'].precision:.4f}, Recall: {rouge_scores['rouge1'].recall:.4f}, F1: {rouge_scores['rouge1'].fmeasure:.4f}")
    print(f"ROUGE-2: Precision: {rouge_scores['rouge2'].precision:.4f}, Recall: {rouge_scores['rouge2'].recall:.4f}, F1: {rouge_scores['rouge2'].fmeasure:.4f}")
    print(f"ROUGE-L: Precision: {rouge_scores['rougeL'].precision:.4f}, Recall: {rouge_scores['rougeL'].recall:.4f}, F1: {rouge_scores['rougeL'].fmeasure:.4f}")
    print(f"\nBERT Scores:")
    print(f"BERT Precision: {bert_precision:.4f}, BERT Recall: {bert_recall:.4f}, BERT F1 Score: {bert_f1:.4f}")
    print("\n" + "-"*50 + "\n")

import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def ensure_full_sentence(text):
    sentences = text.split('. ')
    if len(sentences) > 1 and not text.endswith('.'):
        text = '. '.join(sentences[:-1]) + '.'
    return text.strip()

def generate_summary(article, max_new_tokens=95, min_new_tokens=30, max_words=150):
    inputs = tokenizer.encode("summarize: " + article, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(
        inputs,
        max_new_tokens=max_new_tokens,
        min_new_tokens=min_new_tokens,
        num_beams=2,
        early_stopping=True,
        length_penalty=2.0,
        no_repeat_ngram_size=3
    )
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    summary_words = summary.split()
    if len(summary_words) > max_words:
        summary = " ".join(summary_words[:max_words])
    summary = ensure_full_sentence(summary)
    return summary

def calculate_rouge_scores(reference, generated_summary):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    return scorer.score(reference, generated_summary)

def calculate_bert_scores(reference, generated_summary):
    P, R, F1 = bert_score([generated_summary], [reference], lang="en", rescale_with_baseline=True)
    return P.item(), R.item(), F1.item()


def calculate_vader_scores(text):
    """
    Returns the VADER sentiment scores for a given text.
    """
    return analyzer.polarity_scores(text)
    generated_vader = calculate_vader_scores(generated_summary)
    reference_vader = calculate_vader_scores(reference_summary)

# Paths to your dataset
test_path = '/content/test.csv'

test_df = pd.read_csv(test_path, on_bad_lines='skip', engine='python')


analyzer = SentimentIntensityAnalyzer()

# Load the tokenizer and model
#tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
#model = AutoModelForCausalLM.from_pretrained("google/gemma-2b-it")


results = []

# Loop through the first 30 posts
for index in range(30):
    sample_article = test_df['article'][index]
    reference_summary = test_df['highlights'][index]

    if pd.isna(sample_article) or sample_article.strip() == "":
        print(f"Summary {index}: Article is empty or NaN!")
        continue

    generated_summary = generate_summary(sample_article)

    rouge_scores = calculate_rouge_scores(reference_summary, generated_summary)

    bert_precision, bert_recall, bert_f1 = calculate_bert_scores(reference_summary, generated_summary)

    generated_vader = calculate_vader_scores(generated_summary)
    reference_vader = calculate_vader_scores(reference_summary)

    results.append({
        "Summary": f"Summary {index + 1}",
        "Index": index + 1,
        "Generated Summary": generated_summary,
        "Reference Summary": reference_summary,
        "BERTScore Precision": bert_precision,
        "BERTScore Recall": bert_recall,
        "BERTScore F1": bert_f1,
        "ROUGE-1 Precision": rouge_scores['rouge1'].precision,
        "ROUGE-1 Recall": rouge_scores['rouge1'].recall,
        "ROUGE-1 F1": rouge_scores['rouge1'].fmeasure,
        "ROUGE-2 Precision": rouge_scores['rouge2'].precision,
        "ROUGE-2 Recall": rouge_scores['rouge2'].recall,
        "ROUGE-2 F1": rouge_scores['rouge2'].fmeasure,
        "ROUGE-L Precision": rouge_scores['rougeL'].precision,
        "ROUGE-L Recall": rouge_scores['rougeL'].recall,
        "ROUGE-L F1": rouge_scores['rougeL'].fmeasure,
    })

    print(f"Processed Summary {index + 1}")

results_df = pd.DataFrame(results)
output_path = '/content/cnn_summaries_0_to_30.csv'
results_df.to_csv(output_path, index=False)

print(f"Results successfully saved to {output_path}")

import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def ensure_full_sentence(text):
    sentences = text.split('. ')
    if len(sentences) > 1 and not text.endswith('.'):
        text = '. '.join(sentences[:-1]) + '.'
    return text.strip()

def generate_summary(article, max_new_tokens=95, min_new_tokens=30, max_words=150):
    inputs = tokenizer.encode("summarize: " + article, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(
        inputs,
        max_new_tokens=max_new_tokens,
        min_new_tokens=min_new_tokens,
        num_beams=2,
        early_stopping=True,
        length_penalty=2.0,
        no_repeat_ngram_size=3
    )
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    summary_words = summary.split()
    if len(summary_words) > max_words:
        summary = " ".join(summary_words[:max_words])
    summary = ensure_full_sentence(summary)
    return summary


def calculate_rouge_scores(reference, generated_summary):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    return scorer.score(reference, generated_summary)

def calculate_bert_scores(reference, generated_summary):
    P, R, F1 = bert_score([generated_summary], [reference], lang="en", rescale_with_baseline=True)
    return P.item(), R.item(), F1.item()

def calculate_vader_scores(text):
    return analyzer.polarity_scores(text)

test_path = '/content/test.csv'

test_df = pd.read_csv(test_path, on_bad_lines='skip', engine='python')

analyzer = SentimentIntensityAnalyzer()

# Load the tokenizer and model
#tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
#model = AutoModelForCausalLM.from_pretrained("google/gemma-2b-it")

results = []

for index in range(30):
    sample_article = test_df['article'][index]
    reference_summary = test_df['highlights'][index]


    if pd.isna(sample_article) or sample_article.strip() == "":
        print(f"Summary {index}: Article is empty or NaN!")
        continue


    generated_summary = generate_summary(sample_article)

    rouge_scores = calculate_rouge_scores(reference_summary, generated_summary)

    bert_precision, bert_recall, bert_f1 = calculate_bert_scores(reference_summary, generated_summary)

    generated_vader = calculate_vader_scores(generated_summary)
    reference_vader = calculate_vader_scores(reference_summary)

    results.append({
        "Summary": f"Summary {index + 1}",
        "Index": index + 1,
        "Generated Summary": generated_summary,
        "Reference Summary": reference_summary,
        "BERTScore Precision": bert_precision,
        "BERTScore Recall": bert_recall,
        "BERTScore F1": bert_f1,
        "ROUGE-1 Precision": rouge_scores['rouge1'].precision,
        "ROUGE-1 Recall": rouge_scores['rouge1'].recall,
        "ROUGE-1 F1": rouge_scores['rouge1'].fmeasure,
        "ROUGE-2 Precision": rouge_scores['rouge2'].precision,
        "ROUGE-2 Recall": rouge_scores['rouge2'].recall,
        "ROUGE-2 F1": rouge_scores['rouge2'].fmeasure,
        "ROUGE-L Precision": rouge_scores['rougeL'].precision,
        "ROUGE-L Recall": rouge_scores['rougeL'].recall,
        "ROUGE-L F1": rouge_scores['rougeL'].fmeasure,
        "VADER Generated Compound": generated_vader['compound'],
        "VADER Reference Compound": reference_vader['compound'],
    })

    print(f"Processed Summary {index + 1}")

# Save results to a CSV file
results_df = pd.DataFrame(results)
output_path = '/content/withv_cnn_summaries_0_to_30.csv'
results_df.to_csv(output_path, index=False)

print(f"Results successfully saved to {output_path}")

import pandas as pd
import numpy as np

test_path = '/content/test.csv'

test_df = pd.read_csv(test_path, on_bad_lines='skip', engine='python')

def word_count(text):
    return len(text.split())

test_df['article_length'] = test_df['article'].apply(word_count)
test_df['highlight_length'] = test_df['highlights'].apply(word_count)

average_article_length = np.mean(test_df['article_length'])
average_highlight_length = np.mean(test_df['highlight_length'])

print(f"Average article length: {average_article_length} words")
print(f"Average highlight length: {average_highlight_length} words")

Q1_summary = np.percentile(test_df['highlight_length'], 5)
Q3_summary = np.percentile(test_df['highlight_length'], 95)

print(f"Summary - Q1 (25th percentile): {Q1_summary}")
print(f"Summary - Q3 (75th percentile): {Q3_summary}")

"""# Reddit Dataset Summarization + VADER Analysis via GEMMA 2B-IT"""

import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


file_path = "final_tldr_summary.csv"  #add path
reddit_df = pd.read_csv(file_path)

reddit_df = reddit_df.head(15)

analyzer = SentimentIntensityAnalyzer()

# Load the tokenizer and model
#tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
#model = AutoModelForCausalLM.from_pretrained("google/gemma-2b-it")

def generate_summary(article, max_new_tokens=50, max_words=50):
    inputs = tokenizer.encode("summarize: " + article, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(
        inputs,
        max_new_tokens=max_new_tokens,
        num_beams=2,
        early_stopping=True,
        length_penalty=2.0,
        no_repeat_ngram_size=3
    )
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    summary_words = summary.split()
    if len(summary_words) > max_words:
        summary = " ".join(summary_words[:max_words])
    return summary

def calculate_rouge_scores(reference, generated_summary):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, generated_summary)
    return scores

def calculate_bert_scores(reference, generated_summary):
    P, R, F1 = bert_score([generated_summary], [reference], lang="en", rescale_with_baseline=True)
    return P.item(), R.item(), F1.item()

def calculate_vader_scores(text):
    return analyzer.polarity_scores(text)

results = []

for index, row in reddit_df.iterrows():
    article = str(row.get('selftext', '')).strip()  # full body of the post
    reference_summary = str(row.get('tldr', '')).strip()  # tldr


    if not article:
        print(f"Post {index + 1}: Skipped due to empty content.")
        continue

    generated_summary = generate_summary(article)

    generated_summary = str(generated_summary).strip()

    if reference_summary and generated_summary:
        rouge_scores = calculate_rouge_scores(reference_summary, generated_summary)
        bert_precision, bert_recall, bert_f1 = calculate_bert_scores(reference_summary, generated_summary)
        vader_generated = calculate_vader_scores(generated_summary)
        vader_reference = calculate_vader_scores(reference_summary)

        results.append({
            "Original_Text": article,
            "Generated_Summary": generated_summary,
            "Reference_Summary": reference_summary,
            "ROUGE-1_F1": rouge_scores['rouge1'].fmeasure,
            "ROUGE-2_F1": rouge_scores['rouge2'].fmeasure,
            "ROUGE-L_F1": rouge_scores['rougeL'].fmeasure,
            "BERT_Precision": bert_precision,
            "BERT_Recall": bert_recall,
            "BERT_F1": bert_f1,
            "VADER_Generated_Compound": vader_generated['compound'],
            "VADER_Reference_Compound": vader_reference['compound']
        })
    else:
        print(f"Post {index + 1}: Skipped due to missing reference or generated summary.")

    print(f"Processed post {index + 1}/{len(reddit_df)}")

results_df = pd.DataFrame(results)
results_df.to_csv("reddit_first_15_results.csv", index=False)

print(results_df.head())
print("Analysis complete! Results saved to 'reddit_first_15_results.csv'.")

import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


file_path = "final_parsed_reddit.csv"  # path to the parsed CSV
reddit_df = pd.read_csv(file_path)


reddit_df = reddit_df.head(31)


analyzer = SentimentIntensityAnalyzer()


# Load the model if needed (model is loaded above)
# tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
# model = AutoModelForCausalLM.from_pretrained("google/gemma-2b-it")

def generate_summary(article, max_new_tokens=50, max_words=50):
    inputs = tokenizer.encode("summarize: " + article, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(
        inputs,
        max_new_tokens=max_new_tokens,
        num_beams=2,
        early_stopping=True,
        length_penalty=2.0,
        no_repeat_ngram_size=3
    )
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    summary_words = summary.split()
    if len(summary_words) > max_words:
        summary = " ".join(summary_words[:max_words])
    return summary

def calculate_rouge_scores(reference, generated_summary):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, generated_summary)
    return scores

def calculate_bert_scores(reference, generated_summary):
    P, R, F1 = bert_score([generated_summary], [reference], lang="en", rescale_with_baseline=True)
    return P.item(), R.item(), F1.item()

def calculate_vader_scores(text):
    return analyzer.polarity_scores(text)

results = []

for index, row in reddit_df.iterrows():
    article = str(row.get('selftext', '')).strip()  # full body of the post
    reference_summary = str(row.get('tldr', '')).strip()  # tldr


    if not article:
        print(f"Post {index + 1}: Skipped due to empty content.")
        continue

    generated_summary = generate_summary(article)

    generated_summary = str(generated_summary).strip()

    if reference_summary and generated_summary:
        rouge_scores = calculate_rouge_scores(reference_summary, generated_summary)
        bert_precision, bert_recall, bert_f1 = calculate_bert_scores(reference_summary, generated_summary)
        vader_generated = calculate_vader_scores(generated_summary)
        vader_reference = calculate_vader_scores(reference_summary)


        results.append({
            "Original_Text": article,
            "Generated_Summary": generated_summary,
            "Reference_Summary": reference_summary,
            "ROUGE-1_F1": rouge_scores['rouge1'].fmeasure,
            "ROUGE-2_F1": rouge_scores['rouge2'].fmeasure,
            "ROUGE-L_F1": rouge_scores['rougeL'].fmeasure,
            "BERT_Precision": bert_precision,
            "BERT_Recall": bert_recall,
            "BERT_F1": bert_f1,
            "VADER_Generated_Compound": vader_generated['compound'],
            "VADER_Reference_Compound": vader_reference['compound']
        })
    else:
        print(f"Post {index + 1}: Skipped due to missing reference or generated summary.")


    print(f"Processed post {index + 1}/{len(reddit_df)}")

results_df = pd.DataFrame(results)
results_df.to_csv("reddit_first_31_results.csv", index=False)

print(results_df.head())
print("Analysis complete! Results saved to 'reddit_first_31_results.csv'.")

import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


file_path = "final_tldr_summary.csv"
reddit_df = pd.read_csv(file_path)

reddit_df = reddit_df.head(15)



analyzer = SentimentIntensityAnalyzer()

def generate_summary(article, max_new_tokens=50, max_words=50):
    inputs = tokenizer.encode("summarize: " + article, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(
        inputs,
        max_new_tokens=max_new_tokens,
        num_beams=2,
        early_stopping=True,
        length_penalty=2.0,
        no_repeat_ngram_size=3
    )
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    summary_words = summary.split()
    if len(summary_words) > max_words:
        summary = " ".join(summary_words[:max_words])
    return summary

def calculate_rouge_scores(reference, generated_summary):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, generated_summary)
    return scores

def calculate_bert_scores(reference, generated_summary):
    P, R, F1 = bert_score([generated_summary], [reference], lang="en", rescale_with_baseline=True)
    return P.item(), R.item(), F1.item()


def calculate_vader_scores(text):
    return analyzer.polarity_scores(text)

results = []

for index, row in reddit_df.iterrows():
    article = row['selftext']
    reference_summary = row['tldr']


    if pd.isna(article) or article.strip() == "":
        print(f"Post {index + 1}: Skipped due to empty content.")
        continue


    generated_summary = generate_summary(article)


    rouge_scores = calculate_rouge_scores(reference_summary, generated_summary)
    bert_precision, bert_recall, bert_f1 = calculate_bert_scores(reference_summary, generated_summary)
    vader_generated = calculate_vader_scores(generated_summary)
    vader_reference = calculate_vader_scores(reference_summary)

    results.append({
        "Original_Text": article,
        "Generated_Summary": generated_summary,
        "Reference_Summary": reference_summary,
        "ROUGE-1_F1": rouge_scores['rouge1'].fmeasure,
        "ROUGE-2_F1": rouge_scores['rouge2'].fmeasure,
        "ROUGE-L_F1": rouge_scores['rougeL'].fmeasure,
        "BERT_Precision": bert_precision,
        "BERT_Recall": bert_recall,
        "BERT_F1": bert_f1,
        "VADER_Generated_Compound": vader_generated['compound'],
        "VADER_Reference_Compound": vader_reference['compound']
    })


    print(f"Processed post {index + 1}/{len(reddit_df)}")


results_df = pd.DataFrame(results)
results_df.to_csv("reddit_first_100_results.csv", index=False)


print(results_df.head())
print("Analysis complete! Results saved to 'reddit_first_100_results.csv'.")

def process_and_save_to_csv(start, end, output_csv_path):
    """
    Processes posts in the specified range and outputs results to a CSV file.
    """
    results = []

    for i, (index, row) in enumerate(reddit_df.iloc[start:end].iterrows(), start=start + 1):

        article = row.get('selftext', '')
        reference_summary = row.get('tldr', '')


        if not isinstance(article, str):
            article = str(article) if not pd.isna(article) else ""
        if not isinstance(reference_summary, str):
            reference_summary = str(reference_summary) if not pd.isna(reference_summary) else ""

        # Skip empty posts
        if not article.strip():
            print(f"Post {i}: Skipped due to empty content.\n")
            continue


        generated_summary = generate_summary(article)

        if not generated_summary.strip():
            print(f"Post {i}: Skipped due to empty generated summary.\n")
            continue


        rouge_scores = calculate_rouge_scores(reference_summary, generated_summary)
        bert_precision, bert_recall, bert_f1 = calculate_bert_scores(reference_summary, generated_summary)


        results.append({
            "Summary Index": i,
            "Generated Summary": generated_summary,
            "Reference Summary": reference_summary,
            "BERTScore Precision": bert_precision,
            "BERTScore Recall": bert_recall,
            "BERTScore F1": bert_f1,
            "ROUGE-1 Precision": rouge_scores['rouge1'].precision,
            "ROUGE-1 Recall": rouge_scores['rouge1'].recall,
            "ROUGE-1 F1": rouge_scores['rouge1'].fmeasure,
            "ROUGE-2 Precision": rouge_scores['rouge2'].precision,
            "ROUGE-2 Recall": rouge_scores['rouge2'].recall,
            "ROUGE-2 F1": rouge_scores['rouge2'].fmeasure,
            "ROUGE-L Precision": rouge_scores['rougeL'].precision,
            "ROUGE-L Recall": rouge_scores['rougeL'].recall,
            "ROUGE-L F1": rouge_scores['rougeL'].fmeasure,
        })


    keys = results[0].keys()
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as output_file:
        dict_writer = csv.DictWriter(output_file, fieldnames=keys)
        dict_writer.writeheader()
        dict_writer.writerows(results)

    print(f"Results successfully saved to {output_csv_path}")

def process_posts_and_save_to_csv(start, end, output_csv_path):
    results = []

    for i, (index, row) in enumerate(reddit_df.iloc[start:end].iterrows(), start=start + 1):
        article = row['selftext']
        reference_summary = row['tldr']


        if pd.isna(article) or article.strip() == "":
            print(f"Post {i}: Skipped due to empty content.\n")
            continue


        generated_summary = generate_summary(article)
        rouge_scores = calculate_rouge_scores(reference_summary, generated_summary)
        bert_precision, bert_recall, bert_f1 = calculate_bert_scores(reference_summary, generated_summary)


        results.append({
            "Summary Index": i,
            "Generated Summary": generated_summary,
            "Reference Summary": reference_summary,
            "BERTScore Precision": bert_precision,
            "BERTScore Recall": bert_recall,
            "BERTScore F1": bert_f1,
            "ROUGE-1 Precision": rouge_scores['rouge1'].precision,
            "ROUGE-1 Recall": rouge_scores['rouge1'].recall,
            "ROUGE-1 F1": rouge_scores['rouge1'].fmeasure,
            "ROUGE-2 Precision": rouge_scores['rouge2'].precision,
            "ROUGE-2 Recall": rouge_scores['rouge2'].recall,
            "ROUGE-2 F1": rouge_scores['rouge2'].fmeasure,
            "ROUGE-L Precision": rouge_scores['rougeL'].precision,
            "ROUGE-L Recall": rouge_scores['rougeL'].recall,
            "ROUGE-L F1": rouge_scores['rougeL'].fmeasure,
        })

        print(f"Processed Summary {i}")

    keys = results[0].keys()
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as output_file:
        dict_writer = csv.DictWriter(output_file, fieldnames=keys)
        dict_writer.writeheader()
        dict_writer.writerows(results)

    print(f"Results saved to {output_csv_path}")

start = 0
end = 30
output_csv_path = "/content/reddit_summaries_1_to_30.csv"


process_posts_and_save_to_csv(start, end, output_csv_path)

import pandas as pd
import csv
from transformers import AutoTokenizer, AutoModelForCausalLM
from rouge_score import rouge_scorer
from bert_score import score as bert_score

file_path = '/content/final_tldr_summary.csv'

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

def generate_summary(article, max_new_tokens=50, max_words=50):
    inputs = tokenizer.encode("summarize: " + article, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(
        inputs,
        max_new_tokens=max_new_tokens,
        num_beams=2,
        early_stopping=True,
        length_penalty=2.0,
        no_repeat_ngram_size=3
    )
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    summary_words = summary.split()
    if len(summary_words) > max_words:
        summary = " ".join(summary_words[:max_words])
    return summary

def calculate_rouge_scores(reference, generated_summary):
    return scorer.score(reference, generated_summary)

def calculate_bert_scores(reference, generated_summary):
    P, R, F1 = bert_score([generated_summary], [reference], lang="en", rescale_with_baseline=True)
    return P.item(), R.item(), F1.item()

def process_and_save_to_csv(start, end, output_csv_path):
    results = []
    for i, (index, row) in enumerate(reddit_df.iloc[start:end].iterrows(), start=start + 1):
        article = row['tldr']
        reference_summary = row['selftext']
        if pd.isna(article) or article.strip() == "":
            continue
        generated_summary = generate_summary(article)
        rouge_scores = calculate_rouge_scores(reference_summary, generated_summary)
        bert_precision, bert_recall, bert_f1 = calculate_bert_scores(reference_summary, generated_summary)
        results.append({
            "Summary Index": i,
            "Generated Summary": generated_summary,
            "Reference Summary": reference_summary,
            "BERTScore Precision": bert_precision,
            "BERTScore Recall": bert_recall,
            "BERTScore F1": bert_f1,
            "ROUGE-1 Precision": rouge_scores['rouge1'].precision,
            "ROUGE-1 Recall": rouge_scores['rouge1'].recall,
            "ROUGE-1 F1": rouge_scores['rouge1'].fmeasure,
            "ROUGE-2 Precision": rouge_scores['rouge2'].precision,
            "ROUGE-2 Recall": rouge_scores['rouge2'].recall,
            "ROUGE-2 F1": rouge_scores['rouge2'].fmeasure,
            "ROUGE-L Precision": rouge_scores['rougeL'].precision,
            "ROUGE-L Recall": rouge_scores['rougeL'].recall,
            "ROUGE-L F1": rouge_scores['rougeL'].fmeasure,
        })
    keys = results[0].keys()
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as output_file:
        dict_writer = csv.DictWriter(output_file, fieldnames=keys)
        dict_writer.writeheader()
        dict_writer.writerows(results)

start = 0
end = 30
output_csv_path = "/content/reddit_summaries_1_to_30.csv"

process_and_save_to_csv(start, end, output_csv_path)