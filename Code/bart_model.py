# -*- coding: utf-8 -*-
"""[nlp]_bart_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18izkEuhR5lo8rdJxL6QO5qzqeK22ug6t

## BART Model Code
"""

pip install transformers tensorflow-datasets torch

from google.colab import drive
drive.mount('/content/drive')

!pip install vaderSentiment transformers bert-score rouge-score

from bert_score import score
from rouge_score import rouge_scorer

import numpy as np

def word_count(text):
    if text is None:
        return None
    if len(text.split()) == 0:
      return None
    return len(text.split())

# Filter out None values from both lists
content_length = [word_count(text) for text in content_to_summarize if word_count(text) is not None]
summary_length = [word_count(text) for text in reference_summaries if word_count(text) is not None]

# Calculate max and min lengths only after filtering out None values
max_article_length = np.max(content_length)
min_article_length = np.min(content_length)
max_highlight_length = np.max(summary_length)
min_highlight_length = np.min(summary_length)

print(f"Max article length: {max_article_length} words")
print(f"Min article length: {min_article_length} words")
print(f"Max highlight length: {max_highlight_length} words")
print(f"Min highlight length: {min_highlight_length} words")

Q1_summary = np.percentile(summary_length, 5)
Q3_summary = np.percentile(summary_length, 95)

print(f"Summary - Q1 (25th percentile): {Q1_summary}")
print(f"Summary - Q3 (75th percentile): {Q3_summary}")

"""# BART Summarization for CNN/Dailymail"""

import json
import pandas as pd
from tqdm import tqdm
from transformers import BartTokenizer, BartForConditionalGeneration
from bert_score import score
from rouge_score import rouge_scorer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Load JSON data and extract content + TL;DR
file_path = "/content/drive/MyDrive/NLPFinalProject/Dataset/reddit.json"

# Extract relevant fields: text to summarize and TL;DR
content_to_summarize = []
reference_summaries = []
try:
    with open(file_path, "r", encoding="utf-8") as file:
        for line in file:
            json_object = json.loads(line.strip())
            if 'selftext' in json_object and 'tldr' in json_object:
                content_to_summarize.append(json_object['selftext'])
                reference_summaries.append(json_object['tldr'])

except FileNotFoundError:
    print(f"Error: File not found at {file_path}")

# Generate Summaries Using BART
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

generated_summaries = []
for text in tqdm(content_to_summarize[:100], desc="Generating Summaries"):
    if text:
        inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=50, truncation=True)
        summary_ids = model.generate(inputs, max_length=43, min_length=6, length_penalty=2.0, num_beams=4, early_stopping=True)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)
    else:
        generated_summaries.append(None)

# Filter out invalid summaries (None or empty)
valid_pairs = [
    (gen, ref)
    for gen, ref in zip(generated_summaries, reference_summaries[:100])
    if gen and ref  # Ensure both are not None or empty
]

# Unpack the filtered pairs
if valid_pairs:
    filtered_generated_summaries, filtered_reference_summaries = zip(*valid_pairs)
else:
    filtered_generated_summaries, filtered_reference_summaries = [], []

# Compute BERTScore, ROUGE metrics, and VADER sentiment
if filtered_generated_summaries and filtered_reference_summaries:
    print("Calculating BERTScore, ROUGE, and VADER sentiment analysis...")
    P, R, F1 = score(list(filtered_generated_summaries), list(filtered_reference_summaries), lang="en", verbose=True)

    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_results = [
        rouge_scorer_instance.score(ref, gen)
        for gen, ref in tqdm(zip(filtered_generated_summaries, filtered_reference_summaries), desc="Calculating ROUGE Metrics", total=len(filtered_generated_summaries))
    ]

    vader = SentimentIntensityAnalyzer()
    vader_scores = [
        (vader.polarity_scores(gen)['compound'], vader.polarity_scores(ref)['compound'])
        for gen, ref in tqdm(zip(filtered_generated_summaries, filtered_reference_summaries), desc="Calculating VADER Sentiment", total=len(filtered_generated_summaries))
    ]

    # Save Results to a DataFrame
    data = []
    for i in range(len(filtered_generated_summaries)):
        rouge = rouge_results[i]
        gen_compound, ref_compound = vader_scores[i]
        data.append({
            "Summary Index": i + 1,
            "Generated Summary": filtered_generated_summaries[i],
            "Reference Summary": filtered_reference_summaries[i],
            "BERTScore Precision": P[i].item(),
            "BERTScore Recall": R[i].item(),
            "BERTScore F1": F1[i].item(),
            "ROUGE-1 Precision": rouge['rouge1'].precision,
            "ROUGE-1 Recall": rouge['rouge1'].recall,
            "ROUGE-1 F1": rouge['rouge1'].fmeasure,
            "ROUGE-2 Precision": rouge['rouge2'].precision,
            "ROUGE-2 Recall": rouge['rouge2'].recall,
            "ROUGE-2 F1": rouge['rouge2'].fmeasure,
            "ROUGE-L Precision": rouge['rougeL'].precision,
            "ROUGE-L Recall": rouge['rougeL'].recall,
            "ROUGE-L F1": rouge['rougeL'].fmeasure,
            "Generated VADER Compound": gen_compound,
            "Reference VADER Compound": ref_compound,
        })

    # Create a DataFrame and save it to a CSV file
    results_df = pd.DataFrame(data)
    results_df.to_csv("evaluation_metrics_reddit.csv", index=False)
    print("Evaluation metrics with VADER compound scores saved to evaluation_metrics_with_vader.csv")
else:
    print("No valid summaries to compare!")

"""# BART Summarization for Reddit"""

import json
import pandas as pd
from tqdm import tqdm
import nltk
nltk.download('vader_lexicon')
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from transformers import BartTokenizer, BartForConditionalGeneration
from bert_score import score
from rouge_score import rouge_scorer

# Load CSV data and extract content + Reference Summaries
file_path = "/content/drive/MyDrive/NLPFinalProject/Dataset/cnn_dailymail/test.csv"

# Load the CSV file
try:
    df = pd.read_csv(file_path)
    if 'article' in df.columns and 'highlights' in df.columns:
        content_to_summarize = df['article'].fillna("").tolist()
        reference_summaries = df['highlights'].fillna("").tolist()
    else:
        print("Error: Required columns 'article' and 'highlights' are not present in the CSV.")
        content_to_summarize, reference_summaries = [], []

except FileNotFoundError:
    print(f"Error: File not found at {file_path}")
    content_to_summarize, reference_summaries = [], []

# Generate Summaries Using BART
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

generated_summaries = []
for text in tqdm(content_to_summarize[:100], desc="Generating Summaries"):
    if text:
        inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=100, truncation=True)
        summary_ids = model.generate(inputs, max_length=95, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)
    else:
        generated_summaries.append(None)

# Filter out invalid summaries (None or empty)
valid_pairs = [
    (gen, ref)
    for gen, ref in zip(generated_summaries, reference_summaries[:30])
    if gen and ref
]

# Unpack the filtered pairs
if valid_pairs:
    filtered_generated_summaries, filtered_reference_summaries = zip(*valid_pairs)
else:
    filtered_generated_summaries, filtered_reference_summaries = [], []

# Compute BERTScore
if filtered_generated_summaries and filtered_reference_summaries:
    print("Calculating evaluation metrics...")
    P, R, F1 = score(list(filtered_generated_summaries), list(filtered_reference_summaries), lang="en", verbose=False)

    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_results = [
        rouge_scorer_instance.score(ref, gen)
        for gen, ref in tqdm(zip(filtered_generated_summaries, filtered_reference_summaries), desc="Calculating ROUGE Metrics", total=len(filtered_generated_summaries))
    ]

    vader = SentimentIntensityAnalyzer()
    vader_scores = [
        (vader.polarity_scores(gen)['compound'], vader.polarity_scores(ref)['compound'])
        for gen, ref in tqdm(zip(filtered_generated_summaries, filtered_reference_summaries), desc="Calculating VADER Sentiment", total=len(filtered_generated_summaries))
    ]

    data = []
    for i in range(len(filtered_generated_summaries)):
        rouge = rouge_results[i]
        gen_compound, ref_compound = vader_scores[i]
        data.append({
            "Summary Index": i + 1,
            "Generated Summary": filtered_generated_summaries[i],
            "Reference Summary": filtered_reference_summaries[i],
            "BERTScore Precision": P[i].item(),
            "BERTScore Recall": R[i].item(),
            "BERTScore F1": F1[i].item(),
            "ROUGE-1 F1": rouge['rouge1'].fmeasure,
            "ROUGE-2 F1": rouge['rouge2'].fmeasure,
            "ROUGE-L F1": rouge['rougeL'].fmeasure,
            "Generated VADER Compound": gen_compound,
            "Reference VADER Compound": ref_compound,
        })

    results_df = pd.DataFrame(data)
    results_df.to_csv("evaluation_metrics_cnn.csv", index=False)
    print("Evaluation metrics with summaries and VADER compound scores saved to evaluation_metrics_vader.csv")
else:
    print("No valid summaries to compare!")