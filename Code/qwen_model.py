# -*- coding: utf-8 -*-
"""Qwen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VfgWWkdgiGdAxV6a0g6jZ5yCe7ZRVRur
"""

!pip install vaderSentiment transformers bert-score rouge-score
!pip install tiktoken
!pip install transformers_stream_generator
!pip install bitsandbytes-cuda117

!pip install --upgrade transformers

"""To get CNN data, visit this link: https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail?resource=download. Download test.csv to your local computer. Then drag the file to the "Files" section on the left of your Jupyter Notebook."""

import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from bert_score import score
from rouge_score import rouge_scorer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import nltk
from tqdm import tqdm

nltk.download('vader_lexicon')

file_path = "/content/test.csv"
data = pd.read_csv(file_path)

content_to_summarize = data['article'].tolist()
reference_summaries = data['highlights'].tolist()

# Step 1: Generate Summaries Using the Qwen Model
model_name = "Qwen/Qwen2.5-0.5B-Instruct"
access_token = "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"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=access_token)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, token=access_token)

generated_summaries = []
for text in tqdm(content_to_summarize[:30], desc="Generating Summaries", unit="summary"):
    if text:
        inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=100, truncation=True)
        summary_ids = model.generate(inputs, max_new_tokens=95, min_new_tokens=30, length_penalty=2.0, num_beams=4, early_stopping=True)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)
    else:
        generated_summaries.append(None)

# Step 2: Filter out invalid summaries (None or empty)
valid_pairs = [
    (gen, ref)
    for gen, ref in zip(generated_summaries, reference_summaries[:30])
    if gen is not None and gen.strip() and ref is not None and ref.strip()
]

if valid_pairs:
    filtered_generated_summaries, filtered_reference_summaries = zip(*valid_pairs)
else:
    filtered_generated_summaries, filtered_reference_summaries = [], []

# Step 3: Compute BERTScore, ROUGE, and Vader Sentiment Score
results = []
if filtered_generated_summaries and filtered_reference_summaries:
    P, R, F1 = score(list(filtered_generated_summaries), list(filtered_reference_summaries), lang="en", verbose=True)

    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_results = [
        rouge_scorer_instance.score(ref, gen)
        for gen, ref in zip(filtered_generated_summaries, filtered_reference_summaries)
    ]

    analyzer = SentimentIntensityAnalyzer()
    vader_scores = [
        {
            "generated": analyzer.polarity_scores(gen),
            "reference": analyzer.polarity_scores(ref)
        }
        for gen, ref in zip(filtered_generated_summaries, filtered_reference_summaries)
    ]

    # Step 4: Save Results to DataFrame and CSV
    for i in range(len(filtered_generated_summaries)):
        rouge = rouge_results[i]
        vader = vader_scores[i]
        results.append({
            "Generated Summary": filtered_generated_summaries[i],
            "Reference Summary": filtered_reference_summaries[i],
            "Precision": P[i].item(),
            "Recall": R[i].item(),
            "F1 Score": F1[i].item(),
            "ROUGE-1 Precision": rouge['rouge1'].precision,
            "ROUGE-1 Recall": rouge['rouge1'].recall,
            "ROUGE-1 F1": rouge['rouge1'].fmeasure,
            "ROUGE-2 Precision": rouge['rouge2'].precision,
            "ROUGE-2 Recall": rouge['rouge2'].recall,
            "ROUGE-2 F1": rouge['rouge2'].fmeasure,
            "ROUGE-L Precision": rouge['rougeL'].precision,
            "ROUGE-L Recall": rouge['rougeL'].recall,
            "ROUGE-L F1": rouge['rougeL'].fmeasure,
            "Vader Sentiment Generated": vader['generated'],
            "Vader Sentiment Reference": vader['reference']
        })

    # Convert results to DataFrame
    results_df = pd.DataFrame(results)

    # Save the DataFrame to a CSV file
    results_df.to_csv('summarization_metrics.csv', index=False)
    print("Metrics saved to 'summarization_metrics.csv'.")

else:
    print("No valid summaries to compare!")

import pandas as pd
import ast

df = pd.read_csv('summarization_metrics.csv')

def extract_compound(sentiment_str):
    sentiment_dict = ast.literal_eval(sentiment_str)
    return sentiment_dict['compound']

df['Compound Generated'] = df['Vader Sentiment Generated'].apply(extract_compound)
df['Compound Reference'] = df['Vader Sentiment Reference'].apply(extract_compound)

df.to_csv('summarization_metrics_with_compound.csv', index=False)

print("New CSV file with compound scores has been saved as 'summarization_metrics_with_compound.csv'.")

"""To get Reddit data, visit this link: https://drive.google.com/file/d/1ffWfITKFMJeqjT8loC8aiCLRNJpc_XnF/view. Download tifu_all_tokenized_and_filtered.json to your local computer. Then drag the file to the "Files" section on the left of your Jupyter Notebook."""

import json
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from bert_score import score
from rouge_score import rouge_scorer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Step 1: Load JSON data and extract content + TL;DR
file_path = "/content/tifu_all_tokenized_and_filtered.json"

content_to_summarize = []
reference_summaries = []
try:
    with open(file_path, "r", encoding="utf-8") as file:
        for line in file:
            json_object = json.loads(line.strip())
            if 'selftext' in json_object and 'tldr' in json_object:
                content_to_summarize.append(json_object['selftext'])
                reference_summaries.append(json_object['tldr'])

except FileNotFoundError:
    print(f"Error: File not found at {file_path}")

# Step 2: Generate Summaries Using the Qwen Model
model_name = "Qwen/Qwen2.5-0.5B-Instruct"
access_token = "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"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=access_token)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, token=access_token)

generated_summaries = []
for text in tqdm(content_to_summarize[:100], desc="Generating Summaries"):
    if text:
        inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=50, truncation=True)
        summary_ids = model.generate(inputs, max_new_tokens=43, min_new_tokens=6, length_penalty=2.0, num_beams=4, early_stopping=True)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        generated_summaries.append(summary)
    else:
        generated_summaries.append(None)

# Step 3: Filter out invalid summaries (None or empty)
valid_pairs = [
    (gen, ref)
    for gen, ref in zip(generated_summaries, reference_summaries[:100])
    if gen and ref
]

if valid_pairs:
    filtered_generated_summaries, filtered_reference_summaries = zip(*valid_pairs)
else:
    filtered_generated_summaries, filtered_reference_summaries = [], []

if filtered_generated_summaries and filtered_reference_summaries:
    print("Calculating BERTScore, ROUGE, and VADER sentiment analysis...")
    P, R, F1 = score(list(filtered_generated_summaries), list(filtered_reference_summaries), lang="en", verbose=True)

    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_results = [
        rouge_scorer_instance.score(ref, gen)
        for gen, ref in tqdm(zip(filtered_generated_summaries, filtered_reference_summaries), desc="Calculating ROUGE Metrics", total=len(filtered_generated_summaries))
    ]

    vader = SentimentIntensityAnalyzer()
    vader_scores = [
        (vader.polarity_scores(gen)['compound'], vader.polarity_scores(ref)['compound'])
        for gen, ref in tqdm(zip(filtered_generated_summaries, filtered_reference_summaries), desc="Calculating VADER Sentiment", total=len(filtered_generated_summaries))
    ]

    # Step 4: Save Results to a DataFrame
    data = []
    for i in range(len(filtered_generated_summaries)):
        rouge = rouge_results[i]
        gen_compound, ref_compound = vader_scores[i]
        data.append({
            "Summary Index": i + 1,
            "Generated Summary": filtered_generated_summaries[i],
            "Reference Summary": filtered_reference_summaries[i],
            "BERTScore Precision": P[i].item(),
            "BERTScore Recall": R[i].item(),
            "BERTScore F1": F1[i].item(),
            "ROUGE-1 Precision": rouge['rouge1'].precision,
            "ROUGE-1 Recall": rouge['rouge1'].recall,
            "ROUGE-1 F1": rouge['rouge1'].fmeasure,
            "ROUGE-2 Precision": rouge['rouge2'].precision,
            "ROUGE-2 Recall": rouge['rouge2'].recall,
            "ROUGE-2 F1": rouge['rouge2'].fmeasure,
            "ROUGE-L Precision": rouge['rougeL'].precision,
            "ROUGE-L Recall": rouge['rougeL'].recall,
            "ROUGE-L F1": rouge['rougeL'].fmeasure,
            "Generated VADER Compound": gen_compound,
            "Reference VADER Compound": ref_compound,
        })

    results_df = pd.DataFrame(data)
    results_df.to_csv("evaluation_metrics_reddit.csv", index=False)
    print("Evaluation metrics with VADER compound scores saved to evaluation_metrics_with_vader.csv")
else:
    print("No valid summaries to compare!")

